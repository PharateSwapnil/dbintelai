{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postgresql Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'John', 'Doe', 'john.doe@example.com', '123-456-7890', '123 Main St, Springfield, IL', datetime.datetime(2025, 3, 26, 23, 9, 4, 578904)), (2, 'Jane', 'Smith', 'jane.smith@example.com', '234-567-8901', '456 Oak St, Shelbyville, IL', datetime.datetime(2025, 3, 26, 23, 9, 4, 578904)), (3, 'Mary', 'Johnson', 'mary.johnson@example.com', '345-678-9012', '789 Pine St, Capital City, IL', datetime.datetime(2025, 3, 26, 23, 9, 4, 578904))]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "                    host=\"localhost\",\n",
    "                    port=5432,\n",
    "                    dbname=\"postgres\",\n",
    "                    user=\"postgres\",\n",
    "                    password=\"5010\"\n",
    "                    )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "a = cur.execute(\"\"\"\n",
    "SELECT * FROM customers\n",
    "\"\"\")\n",
    "\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs diff invocation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "\n",
      "content='Hi there! How can I help you today?\\n' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-663fc6a0-abc7-47b0-8a89-687b21567bf0-0'\n",
      "generations=[[ChatGeneration(text='Hi there! How can I help you today?\\n', generation_info={'finish_reason': 'STOP', 'safety_ratings': []}, message=AIMessage(content='Hi there! How can I help you today?\\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a6a58efc-27a1-4487-8b2b-211e42243400-0'))], [ChatGeneration(generation_info={'finish_reason': 'RECITATION', 'safety_ratings': []}, message=AIMessage(content='', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'RECITATION', 'safety_ratings': []}, id='run-211b7007-5597-4289-a2e0-c693fc1004d2-0'))]] llm_output={} run=[RunInfo(run_id=UUID('a6a58efc-27a1-4487-8b2b-211e42243400')), RunInfo(run_id=UUID('211b7007-5597-4289-a2e0-c693fc1004d2'))]\n",
      "Response saved to response.json\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import os\n",
    "\n",
    "# Function to get LLM\n",
    "def get_llm(model_choice: str):\n",
    "    \"\"\"Get LLM based on model choice.\"\"\"\n",
    "    if model_choice == \"gemini\":\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    "        )\n",
    "    elif model_choice == \"llama3\":\n",
    "        return ChatGroq(\n",
    "            api_key=os.getenv('GROQ_API_KEY'),\n",
    "            model_name=\"llama3-70b-8192\",\n",
    "            temperature=0.1,\n",
    "            streaming=True\n",
    "        )\n",
    "    else:\n",
    "        # Default to Gemini\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "\n",
    "# Choose the model you want to use\n",
    "model_choice = \"gemini\"  # or \"llama3\"\n",
    "\n",
    "# Get the corresponding LLM instance\n",
    "llm = get_llm(model_choice)\n",
    "\n",
    "# Generate a response using the ChatGoogleGenerativeAI instance\n",
    "# example usage of llm.predict(\"hi\")\n",
    "response = llm.predict(\"hi\")\n",
    "# Print the response\n",
    "print(response)\n",
    "\n",
    "\n",
    "## example usage of llm.invoke(\"hi\")\n",
    "response = llm.invoke(\"hi\")\n",
    "print(response) \n",
    "\n",
    "\n",
    "## example usage of llm.generate(messages)\n",
    "messages = [\n",
    "    [HumanMessage(content=\"hi\")],\n",
    "    [AIMessage(content=\"Hello! How can I help you today?\")]  # Assistant's response\n",
    "]\n",
    "\n",
    "# Generate a response using the conversation history\n",
    "response = llm.generate(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postres final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products LIMIT 1\n",
      "\n",
      "   product_id   name                  description                  price    \\\n",
      "0       1      Laptop  15-inch laptop with 16GB RAM and 512GB SSD  1200.00   \n",
      "\n",
      "   stock_quantity         created_at          \n",
      "0        50       2025-03-26 23:09:16.665656  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "# from ai_models import get_llm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv() \n",
    "\n",
    "def get_llm(model_choice: str):\n",
    "    \"\"\"Get LLM based on model choice.\"\"\"\n",
    "    if model_choice == \"gemini\":\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        )\n",
    "    elif model_choice == \"llama3\":\n",
    "        return ChatGroq(\n",
    "            api_key=os.getenv('GROQ_API_KEY'),\n",
    "            model_name=\"llama3-70b-8192\",\n",
    "            temperature=0.1,\n",
    "            streaming=True\n",
    "        )\n",
    "    else:\n",
    "        # Default to Gemini\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "        )\n",
    "    \n",
    "class PostgresDBQueryHandler:\n",
    "    def __init__(self, host, port, dbname, user, password):\n",
    "        \"\"\"Initialize the PostgreSQL connection.\"\"\"\n",
    "        self.conn = psycopg2.connect(\n",
    "            host=host, port=port, dbname=dbname, user=user, password=password\n",
    "        )\n",
    "        self.cursor = self.conn.cursor()\n",
    "      \n",
    "    def get_schema_info(self):\n",
    "        \"\"\"Fetch schema details for all schemas in the database.\"\"\"\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT table_schema, table_name, column_name, data_type\n",
    "        FROM information_schema.columns;\n",
    "        \"\"\"\n",
    "        self.cursor.execute(query)\n",
    "        schema_info = self.cursor.fetchall()\n",
    "        organized_schema = {}\n",
    "        \n",
    "        for schema, table, column, dtype in schema_info:\n",
    "            if schema not in organized_schema:\n",
    "                organized_schema[schema] = {}\n",
    "            if table not in organized_schema[schema]:\n",
    "                organized_schema[schema][table] = {}\n",
    "            \n",
    "            organized_schema[schema][table][column] = dtype \n",
    "\n",
    "        return organized_schema\n",
    "    \n",
    "    def generate_sql_query(self, model_choice, prompt, schema_info):\n",
    "        \"\"\"Generate accurate SQL queries using structured schema info.\"\"\"\n",
    "        # # Initialize schema_text variable\n",
    "        schema_text = \"\"\n",
    "        # Check if 'public' schema exists in schema_info\n",
    "        if 'public' in schema_info:\n",
    "            # Iterate only over the 'public' schema\n",
    "            schema_text += f\"Schema: public\\n\"\n",
    "            public_schema = schema_info['public']\n",
    "            # Iterate over the tables in the 'public' schema\n",
    "            for table, columns in public_schema.items():\n",
    "                columns_text = \", \".join([f\"{col}: {dtype}\" for col, dtype in columns.items()])\n",
    "                schema_text += f\"Table: {table}\\nColumns: {{{columns_text}}}\\n\"\n",
    "\n",
    "        llm = get_llm(model_choice)\n",
    "\n",
    "        full_prompt = f\"\"\"\n",
    "        Analyze the following user prompt and generate a valid SQL query optimized for the given database schema.\n",
    "        \n",
    "        Database Schema Information:\n",
    "        {schema_text}\n",
    "        \n",
    "        User Query: {prompt}\n",
    "        \n",
    "        Instructions:\n",
    "        - dont generate CREATE/ ALTER / DELETE / UPDATE Sql statements\n",
    "        - If the query involves multiple SELECT statements with UNION or UNION ALL, make sure each SELECT statement returns the same number of columns.\n",
    "        - If necessary, add NULL placeholders to ensure consistency in the number of columns for each SELECT.\n",
    "        - Do not include any explanations or additional text. Only return the final SQL query.\n",
    "        - dont show ```sql in output\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the initial SQL query\n",
    "        response = llm.predict(full_prompt)\n",
    "\n",
    "        # Clean the response by removing any extra text\n",
    "        # In this case, you could strip unwanted parts if needed, but the prompt should ideally make this unnecessary\n",
    "        query = response.strip()\n",
    "\n",
    "        optimised_query = f\"\"\"As an SQL Developer analyse the {query} and return optimized query if necessary based on the query string\n",
    "                            Intruction:\n",
    "                            remove \"```\" from the query string and return the optimized query\n",
    "                            \"\"\"\n",
    "\n",
    "        final_response = llm.predict(full_prompt)\n",
    "\n",
    "        # Returning the generated SQL query without any extra text\n",
    "        return final_response\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Run the SQL query and return results as a table.\"\"\"\n",
    "        self.cursor.execute(query)\n",
    "        columns = [desc[0] for desc in self.cursor.description]\n",
    "        results = self.cursor.fetchall()\n",
    "        return pd.DataFrame(results, columns=columns)  # Return DataFrame for tabular format\n",
    "    \n",
    "    def visualize_data(self, data):\n",
    "        \"\"\"Visualize the data using Sweetviz.\"\"\"\n",
    "        # report = sv.analyze(data)\n",
    "        # report.show_html('sweetviz_report.html')\n",
    "\n",
    "        # from autoviz import AutoViz_Class\n",
    "        # AV = AutoViz_Class()\n",
    "        # return AV.AutoViz(query_result)\n",
    "\n",
    "        import dtale\n",
    "        d = dtale.show(data)\n",
    "        return d.open_browser()\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize DB connection (Use credentials from main.py)\n",
    "    db = PostgresDBQueryHandler(\n",
    "                    host=\"localhost\",\n",
    "                    port=5432,\n",
    "                    dbname=\"postgres\",\n",
    "                    user=\"postgres\",\n",
    "                    password=\"5010\"\n",
    "                    )\n",
    "    \n",
    "    # Fetch Schema Info\n",
    "    schema_info = db.get_schema_info()\n",
    "\n",
    "    # print(schema_info)\n",
    "    \n",
    "    # Generate SQL Query\n",
    "    user_prompt = \"helloe\"\n",
    "    generated_sql = db.generate_sql_query(\n",
    "                                        model_choice=\"gemini-1.5-flash\",\n",
    "                                        prompt=user_prompt,\n",
    "                                        schema_info=schema_info\n",
    "                                        )\n",
    "    \n",
    "    print(generated_sql)\n",
    "    \n",
    "    # Execute SQL Query\n",
    "    query_result = db.execute_query(generated_sql)\n",
    "    print(query_result)\n",
    "    \n",
    "    # Visualize Data\n",
    "    db.visualize_data(query_result)\n",
    "    \n",
    "    # Close Connection\n",
    "    db.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
